[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "深層学習ノート",
    "section": "",
    "text": "Date\n\n\nTitle\n\n\nCategories\n\n\n\n\n\n\n2024-11-3\n\n\nMNIST画像分類モデルを実装してみよう\n\n\nkaggle, mnist, fastai\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "posts/mnist_kaggle/index.html",
    "href": "posts/mnist_kaggle/index.html",
    "title": "MNIST画像分類モデルを実装してみよう",
    "section": "",
    "text": "このノートでは、kaggleのMNIST(手書き数字データ)を用いた画像分類を行なっていきます。深層学習にはfastaiライブラリーを使用します。MNIST手書き数字データは、機械学習界隈のHello Worldのような、誰もが最初に通るデータセットです。ここでは複雑なモデルは用いず、三層のモデルとresnet18モデルの二つを使用します。目標は99% accuracyで数字を分類できるモデルをつくることです。対象読者はkaggleの画像コンペの始め方が分からないけど興味がある人です。MNIST画像は一見簡単そうに見えますが、初学者には取っ付きにくいものです。特に、学習に時間がかかる問題にどのように対処するのか、モデルの成果をどう測るのかといった詰まりポイントを重点的に見ていきます。逆にモデル設計の詳細(畳み込み層の動きなど)は見ていきません。\n–\n実装の流れは以下の通りです。\n\n必要なライブラリのインストール\nデータセットのインストール\nトレーニングデータとテストデータの差異を確認\nサンプルデータ上でのトレーニング\nトレーニング結果の妥当性を確認\nデータセット全体でのトレーニング\nまとめ\n参照ページ\n\n\n以下のコードは実行環境にCUDA(GPU)が使用可能かを確認します。このノートはGPUが使えることを前提に動いています。kaggleの上でも無料でGPUが使えますが、クラウドのサービス(paperspace, google colabなど)の方がモデルの学習を早くできるのでお勧めです。\n\n!nvidia-smi\n\nThu Oct 31 02:47:40 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n| 41%   37C    P8    16W / 140W |      1MiB / 16376MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\nfastkaggleはfastaiとkaggle apiを合わせたライプラリーです。ここで用いられている全インポートimport *は、fastai推奨のインポート方法です。私自身、初めは慣れずに居心地が悪い表記方法でしたが今は慣れました。jupyter notebookなどの実験(仮説と検証を繰り返す)環境では全インポートで問題ありません。\n\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n\n\n\nまずkaggleのコンペ参加には同意が必要になります。同意が済んだら、次はapiキーの取得です。/root/.config/kaggle/kaggle.jsonファイルに取得したapiキーを記述します。ファイルは以下のコードで作成します。\n\n!mkdir /root/.config/kaggle\n!touch /root/.config/kaggle/kaggle.json\n\n\n\n\nsetup_comp関数でデータセットをdigit-recognizerフォルダーにインストールします。ここでエラーが出た場合は、もう一度apiキーが設定されているか、競技の同意が済んでいるかを見てみます。よくあるエラー原因は競技名のスペルミスです。\n\ncomp = \"digit-recognizer\"\n\npath = setup_comp(comp)\npath.ls()\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\nDownloading digit-recognizer.zip to /notebooks\n\n\n100%|██████████| 15.3M/15.3M [00:00&lt;00:00, 74.5MB/s]\n\n\n\n\n\n(#3) [Path('digit-recognizer/sample_submission.csv'),Path('digit-recognizer/test.csv'),Path('digit-recognizer/train.csv')]\n\n\nトレーニングデータは42000、テストデータは28000サンプルあることがわかります。\n\ntrn_df = pd.read_csv(path/\"train.csv\")\ntst_df = pd.read_csv(path/\"test.csv\")\nsmp_df = pd.read_csv(path/\"sample_submission.csv\")\n\ntrn_df.shape, tst_df.shape, smp_df.shape\n\n((42000, 785), (28000, 784), (28000, 2))\n\n\n\n\n\n最初の行(label行)にはデータのラベルが、その他の行(pixel0 - 783行)にはピクセルデータが格納されていることがわかります。このデータは色(color channel)の無い白黒画像であることが推測できます。\n\ntrn_df.head(n=3)\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n3 rows × 785 columns\n\n\n\nラベルは0から9の10ラベルです。それぞれ約4000個と、おおよそ均等に配分されたデータセットであることが分かります。\n\ntrn_df[\"label\"].unique()\n\narray([1, 0, 4, 7, 3, 5, 8, 9, 2, 6])\n\n\n\ntrn_df.groupby(\"label\").size().plot.barh();\n\n\n\n\n\n\n\n\nラベルとピクセルデータ(画像)が対応していることを確認します。\n\nidx = 0\nlabel, img_val = trn_df.iloc[idx, 0], trn_df.iloc[idx, 1:].values\n\nshow_image(img_val.reshape((28, 28)), title=label);\n\n\n\n\n\n\n\n\n\n\n\ntrainとtestフォルダそれぞれに画像データを保存します。\n\n!mkdir {path}/train\n!mkdir {path}/test\npath.ls()\n\n(#5) [Path('digit-recognizer/train'),Path('digit-recognizer/sample_submission.csv'),Path('digit-recognizer/test.csv'),Path('digit-recognizer/train.csv'),Path('digit-recognizer/test')]\n\n\n\ndef save_img(img, f_path):\n    img = img.reshape((28, 28))\n    img = Image.fromarray(img.astype(\"uint8\"))\n    img.save(f_path)\n\n\nfor idx, row in trn_df.iterrows():\n    label, img = row[0], row[1:]\n\n    f_path = f\"{path}/train/{label}_{idx}.jpg\"\n    save_img(img.values, f_path)\n\n\n(path/\"train\").ls()\n\n(#42000) [Path('digit-recognizer/train/7_19624.jpg'),Path('digit-recognizer/train/2_40612.jpg'),Path('digit-recognizer/train/3_32852.jpg'),Path('digit-recognizer/train/1_13520.jpg'),Path('digit-recognizer/train/9_29764.jpg'),Path('digit-recognizer/train/6_16260.jpg'),Path('digit-recognizer/train/9_18518.jpg'),Path('digit-recognizer/train/0_13218.jpg'),Path('digit-recognizer/train/5_32574.jpg'),Path('digit-recognizer/train/7_20089.jpg')...]\n\n\n\nfor idx, img in tst_df.iterrows():\n    f_path = f\"{path}/test/{idx}.jpg\"\n    save_img(img.values, f_path)\n\n\n(path/\"test\").ls()\n\n(#28000) [Path('digit-recognizer/test/13420.jpg'),Path('digit-recognizer/test/22873.jpg'),Path('digit-recognizer/test/6022.jpg'),Path('digit-recognizer/test/13644.jpg'),Path('digit-recognizer/test/1332.jpg'),Path('digit-recognizer/test/3737.jpg'),Path('digit-recognizer/test/617.jpg'),Path('digit-recognizer/test/10239.jpg'),Path('digit-recognizer/test/14546.jpg'),Path('digit-recognizer/test/4200.jpg')...]\n\n\n\n\n\n\nまず分類モデルを作る前に、二つのデータセット(トレーニング・テスト)に違いがないこと確認します。例えばトレーニングセットには0から9までの数字画像が、テストセットには10から19までの数字といった全く違う種類のデータが入っているかもしれません。二つのデータセットに差異がないことは、この後の分類モデルを作る際の前提になります。 二つのデータセットの差異確認は深層学習モデルを用いて行います。ここでのラベルはtrain/testとなり、モデルは画像がどちらのデータセットに属しているのかを予測します。全データセット70000に対して、トレーニングは42000、テストは28000あります。もし、モデルが0.6 accuracy(42000/70000=0.6)から離れた値を出す場合、二つのデータセットは異なる種類のものであると推測できます。\n\ndef label_func(f_path): return f_path.parent.name\n\nlabel_func((path/\"train\").ls()[0]), label_func((path/\"test\").ls()[0])\n\n('train', 'test')\n\n\n\ndef get_db(size=28):\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock),\n        get_items=get_image_files,\n        get_y=label_func,\n        splitter=RandomSplitter(),\n        item_tfms=Resize(size),\n        batch_tfms=[*aug_transforms(do_flip=False, min_scale=0.8), Normalize.from_stats(*imagenet_stats)]\n    )\n\nmnist = get_db()\n\n学習用のデータセットを用意する場合、全トレーニングデータを使う必要はありません。アイデアの有効性を試す実験段階では、開発サイクルを高速で回すためにも少ないサンプル数を使用します。 ここではランダムに、トレーニングデータセットから10%のデータを取得しています。どのようにサンプルデータを用意するのかは様々な手法がありますが、今回のような画像分類モデルでは、各ラベルのサンプル数が300ほどあれば実験用途として十分な気がします。以下のdls.show_batchによる画像の表示は、ざっとデータを理解するのに役立ちます。トレーニングの前段階で、トレーニングデータ(画像+対応するラベル)を確認するのは必須ステップです。\n\ndef get_dls(db, path, sample=1, bs=256):\n    # https://knowing.net/posts/2022/05/fastai-dataloaders/\n    dls = db.dataloaders(path, bs=bs)\n    selected_items = random.sample(dls.train_ds.items,  int(len(dls.train_ds.items)*sample))\n    dls.train = dls.test_dl(selected_items, with_labels=True)\n    return dls\n\ndls = get_dls(mnist, path, sample=0.1)\ndls.show_batch(max_n=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\nモデルの設計は入力/出力を中心に行います。fastaiはPytorchの上に作られたライブラリーなので、画像型は(N, C, H, W)、つまり(バッチ数, カラーチャンネル, 縦, 横)としてモデルに入力されます。モデルの出力はデータセットのラベル数dls.cによって決まります。ここではモデルは画像がトレーニング・テストセットのどちらに属しているのかを予測します。\n\ndef get_model():\n    return nn.Sequential(\n        nn.Conv2d(3, 1, 1),\n        # nn.Upsample(size=(28, 28)), # Needs Upsample layer if you'll use learn.tta() later\n        nn.Flatten(1),\n        nn.Dropout(0.1),\n        nn.Linear(28*28, 100),\n        nn.BatchNorm1d(100),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(100, dls.c)\n    )\n\nget_model()\n\nSequential(\n  (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Dropout(p=0.1, inplace=False)\n  (3): Linear(in_features=784, out_features=100, bias=True)\n  (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): Dropout(p=0.2, inplace=False)\n  (6): ReLU()\n  (7): Linear(in_features=100, out_features=2, bias=True)\n)\n\n\n学習に必要なデータセットの用意・モデルの設計が済んだら、いよいよトレーニングの開始です。learn.lr_find関数で学習率を求めたらトレーニングサイクルを回します。\n\nlearn = Learner(dls, get_model(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.015848932787775993)\n\n\n\n\n\n\n\n\n\n結果はおおよそ59% accuracyで、画像がどちらのデータセットに属しているのかを予測しています。二つのデータセットに差異はなさそうです。\n\nlearn.fit_one_cycle(4, lr_max=slice(3e-3, 3e-2))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.710549\n0.795406\n0.597000\n00:13\n\n\n1\n0.700336\n0.704644\n0.568500\n00:12\n\n\n2\n0.688758\n0.678533\n0.596929\n00:12\n\n\n3\n0.674994\n0.681112\n0.586357\n00:11\n\n\n\n\n\n\n\n\nでは、本題の画像分類モデルのトレーニングに移っていきます。サンプルデータの用意・画像の表示を行い、モデルの学習率を求めます。先のデータと同じ画像データを用いますが、今回は対応するラベルが異なります。ここでは、モデルは画像が0から9までのどの数字であるかを予測します。\n\ndef label_func(f_path): return f_path.name.split(\"_\")[0]\n\n(path/\"train\").ls()[0], label_func((path/\"train\").ls()[0])\n\n(Path('digit-recognizer/train/7_19624.jpg'), '7')\n\n\n\nmnist = get_db()\ndls = get_dls(mnist, path/\"train\", sample=0.1)\ndls.show_batch(max_n=30, nrows=3, ncols=10)\n\n\n\n\n\n\n\n\n\nlearn = Learner(dls, get_model(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\n\n\n\nおおよそ0.91 accuracyの精度のモデルを、10%のトレーニングデータで作りました。この時点でトレーニングデータを増やし、モデルの精度を高めようとする方法がありますが、これは間違いです。トレーニングデータ量を増やすことは過学習を防ぐのに有効ですが、学習不足には違う対処が求められます。 まずは他のモデルの精度と比較して、学習済みのモデルが過学習か学習不足かを見ていきます。ここではresnet18を比較モデルとします。\n\nlearn.fit_one_cycle(6, lr_max=slice(3e-3, 1e-2))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.595407\n1.169721\n0.635357\n00:07\n\n\n1\n1.004385\n0.500168\n0.851667\n00:03\n\n\n2\n0.707560\n0.360822\n0.888214\n00:03\n\n\n3\n0.542243\n0.318499\n0.904405\n00:03\n\n\n4\n0.434244\n0.306300\n0.910000\n00:03\n\n\n5\n0.363178\n0.299947\n0.909881\n00:03\n\n\n\n\n\n\nlearn = Learner(dls, resnet18(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n先のモデルと比較して、resnet18はより精度の高いモデルであることが分かります。モデル精度の比較結果から、先のモデルは学習不足であったことが分かります。一般的に、過学習になる前段階で注意するポイントは学習不足です。学習不足にはより複雑なモデルが有効です。トレーニングサイクルを増やす、学習率を少し上げるなども学習不足に効果的です。よくある間違いは、過学習になる前(学習不足段階)に過学習を防ぐ手法(データ量を増やす、データ拡張、regularizationなど)を用いることです。過学習への対処は、過学習が起きた後に行います。起きる前、予防的には行いません。\n\nlearn.fine_tune(6, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.481695\n1.180103\n0.615119\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.227101\n0.339515\n0.909167\n00:03\n\n\n1\n0.168335\n0.627921\n0.838571\n00:04\n\n\n2\n0.143743\n0.432962\n0.900833\n00:03\n\n\n3\n0.120995\n0.268689\n0.937381\n00:03\n\n\n4\n0.091097\n0.148554\n0.959881\n00:03\n\n\n5\n0.066370\n0.122915\n0.967143\n00:03\n\n\n\n\n\nトレーニング・検証データそれぞれの損失を見てきます。トレーニングデータの損失はスムーズな減少が見られますが、検証データの損失はスムーズではない形で推移しています。一つ考えられる原因は、重みの更新が急であることです。スムーズな損失の推移には、いくつかの正則化テクニックが効果的です。weight decayを後のトレーニングで試していきたいと思います。\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n\n学習済みのモデルが数字を正しく予測しているか、検証画像(学習していないデータ)を用いて見ていきます。おおよそ、モデルは正しく手書き画像を分類できていることが分かります。\n\nlearn.show_results(max_n=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n逆に、モデルがどの数字画像を正しく予測できていないかを見ていきます。2と7、4と9の数字ペアの分類が、モデルの最も不得意とする画像であることが分かります。\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.most_confused(min_val=15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[('2', '7', 20), ('4', '9', 19)]\n\n\nここではモデルが実際に間違えた予測をしている画像を見ていきます。ざっくりと見た時、モデルが間違えた画像は人間が見ても間違えそうな、紛らわしい数字であることが見て取れます。この紛らわしいラベルへの対処方としては、label smoothingの導入があります。\n\ninterp.plot_top_losses(k=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nでは先のトレーニング結果を踏まえて、再度トレーニングをしていきます。モデル・データセットは同様に、label smoothing・weight decayを新たに使用します。label smoothing関数はラベルのノイズを抑え損失を滑らかにしますが、より多くのトレーニングサイクルを必要とします。先のトレーニングとの相違点は、accuracyの精度は上がっている一方、lossが以前より高い値であることです。ここでの注意点は、accuracyの精度を見ること・lossは参考程度に見ることです。lossはモデルがトレーニングしやすいように設計された関数の出力です。見るべき大事なポイント(モデルの精度)はaccuracyです。\n\nlearn = Learner(dls, resnet18(), loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy).to_fp16()\nlearn.fine_tune(9, 2e-3, wd=0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.378253\n2.647140\n0.526190\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.336900\n1.490081\n0.937024\n00:03\n\n\n1\n1.207181\n1.428404\n0.939048\n00:03\n\n\n2\n1.151014\n1.397945\n0.923333\n00:03\n\n\n3\n1.131866\n1.395567\n0.904762\n00:03\n\n\n4\n1.111709\n1.244161\n0.941310\n00:03\n\n\n5\n1.094381\n1.210672\n0.945476\n00:03\n\n\n6\n1.079392\n1.151900\n0.958809\n00:03\n\n\n7\n1.064843\n1.112762\n0.970357\n00:03\n\n\n8\n1.053271\n1.109932\n0.971310\n00:03\n\n\n\n\n\nモデルの精度・loss推移の滑らかさともに向上しています。\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n今度は全トレーニングデータを用いて学習サイクルを回します。ここでのポイントは、ぼーっと学習の進行を見ない・ノートブックから離れることです。\n\ndls = get_dls(mnist, path/\"train\", sample=1) # Use all training image samples\nlearn = Learner(dls, resnet18(), loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0006918309954926372)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(15, 1e-3, wd=0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.352898\n1.110789\n0.974286\n00:32\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.050016\n1.066425\n0.983810\n00:33\n\n\n1\n1.039457\n1.074759\n0.982262\n00:33\n\n\n2\n1.050062\n1.090605\n0.977381\n00:33\n\n\n3\n1.048530\n1.073222\n0.982857\n00:35\n\n\n4\n1.041486\n1.075171\n0.981310\n00:34\n\n\n5\n1.038663\n1.071839\n0.983929\n00:35\n\n\n6\n1.031112\n1.061485\n0.986905\n00:36\n\n\n7\n1.026435\n1.066055\n0.985714\n00:34\n\n\n8\n1.025973\n1.065412\n0.985833\n00:34\n\n\n9\n1.020517\n1.052722\n0.990000\n00:35\n\n\n10\n1.018529\n1.049975\n0.989643\n00:35\n\n\n11\n1.016169\n1.046174\n0.991429\n00:34\n\n\n12\n1.015739\n1.046068\n0.991071\n00:34\n\n\n13\n1.015518\n1.045375\n0.991548\n00:40\n\n\n14\n1.015467\n1.045369\n0.991548\n00:42\n\n\n\n\n\n\n\n\nでは、学習済みモデルの精度をテストデータで評価していきます。まずはテストファイルを名前順にソートし、テスト画像(ラベルは含まれていない)を表示します。learn.dls.test_dlは、トレーニング画像と同じ下処理を、テスト画像に行う便利な関数です。\n\n# https://stackoverflow.com/questions/33159106/sort-filenames-in-directory-in-ascending-order\nordered_tst_files = get_image_files(path/\"test\").sorted(key=lambda f:  int(re.findall(\"(\\d+).jpg$\", f.name)[0]))\nordered_tst_files[:10]\n\n(#10) [Path('digit-recognizer/test/0.jpg'),Path('digit-recognizer/test/1.jpg'),Path('digit-recognizer/test/2.jpg'),Path('digit-recognizer/test/3.jpg'),Path('digit-recognizer/test/4.jpg'),Path('digit-recognizer/test/5.jpg'),Path('digit-recognizer/test/6.jpg'),Path('digit-recognizer/test/7.jpg'),Path('digit-recognizer/test/8.jpg'),Path('digit-recognizer/test/9.jpg')]\n\n\n\ntest_dl = learn.dls.test_dl(ordered_tst_files)\ntest_dl.show_batch(max_n=10, nrows=1, ncols=10)\n\n\n\n\n\n\n\n\n学習済みモデルにテスト画像を予測(分類)させます。\n\npreds = learn.get_preds(dl=test_dl, with_decoded=True)\npreds\n\n\n\n\n\n\n\n\n(tensor([[7.9680e-05, 5.4073e-05, 8.9991e-01,  ..., 9.6678e-05, 9.9019e-05,\n          1.0097e-04],\n         [8.9885e-01, 7.8046e-05, 1.3502e-04,  ..., 1.0204e-04, 1.0433e-04,\n          9.8660e-05],\n         [7.4821e-05, 1.0368e-04, 1.0127e-04,  ..., 1.0342e-04, 1.0242e-04,\n          1.0302e-04],\n         ...,\n         [6.9998e-05, 5.2941e-05, 2.7252e-05,  ..., 1.0606e-04, 1.0679e-04,\n          9.8137e-05],\n         [9.9981e-05, 8.6992e-05, 9.6574e-05,  ..., 1.0136e-04, 1.0376e-04,\n          9.7474e-05],\n         [5.9522e-05, 4.7224e-05, 9.0107e-01,  ..., 8.9877e-05, 9.8856e-05,\n          9.8278e-05]]),\n None,\n tensor([2, 0, 9,  ..., 3, 9, 2]))\n\n\n予測した数字が妥当かどうか、画像と一緒に見比べてみます。モデルはテスト画像の数字を分類できていそうです。\n\nprint(preds[2][:10])\ntest_dl.show_batch(max_n=10, nrows=1, ncols=10)\n\ntensor([2, 0, 9, 0, 3, 7, 0, 3, 0, 3])\n\n\n\n\n\n\n\n\n\n予測した値をcsvファイルに保存し、kaggleに提出します。\n\nsmp_df[\"Label\"] = preds[2].apply_(lambda x: int(learn.dls.vocab[x]))\nsmp_df.to_csv(path/\"submission.csv\", index=False)\n!head {path}/submission.csv\n\nImageId,Label\n1,2\n2,0\n3,9\n4,0\n5,3\n6,7\n7,0\n8,3\n9,0\n\n\n\nfrom kaggle import api\napi.competition_submit_cli(path/\"submission.csv\", \"first model\", comp)\n\n100%|██████████| 208k/208k [00:00&lt;00:00, 657kB/s]\n\n\nSuccessfully submitted to Digit Recognizer\n\n\n最後にインストールしたzipファイルと画像ファイルを削除します。\n\n!rm -rf {path}\n!rm -rf digit-recognizer.zip\n\n\n\n\n\n\n\n\nkaggle の結果\n\n\n目標値99% accuracyを達成することができました。今回のポイント三点をまとめていきます。\n\n少ないサンプル数を使用して、高速な開発サイクルを回す。 → 全サンプルを使用するのは、一番最後に。\n過学習の対処は過学習が起きた後に行う。 → 予防的には行わない。先に注意するのは学習不足とその対処。\nモデル精度の評価はメトリックを見る。 → 損失の推移は参考程度に。\n\nもしも読者がこのノートを役に立ったと思ったら、リアクションボタンを押してもらえると幸いです。質問や間違いがあれば、以下コメント欄に書き込んでください。\n\n\n\n\nkaggle公式競技ページ\nmnistデータセットのwiki\nfastaiのクリエイター(Jeremy Howardさん)のノートブック\n他の競技参加者(Jacopo Repossiさん)のノートブック"
  },
  {
    "objectID": "posts/mnist_kaggle/index.html#hello-world-in-deep-learning",
    "href": "posts/mnist_kaggle/index.html#hello-world-in-deep-learning",
    "title": "MNIST画像分類モデルを実装してみよう",
    "section": "",
    "text": "このノートでは、kaggleのMNIST(手書き数字データ)を用いた画像分類を行なっていきます。深層学習にはfastaiライブラリーを使用します。MNIST手書き数字データは、機械学習界隈のHello Worldのような、誰もが最初に通るデータセットです。ここでは複雑なモデルは用いず、三層のモデルとresnet18モデルの二つを使用します。目標は99% accuracyで数字を分類できるモデルをつくることです。対象読者はkaggleの画像コンペの始め方が分からないけど興味がある人です。MNIST画像は一見簡単そうに見えますが、初学者には取っ付きにくいものです。特に、学習に時間がかかる問題にどのように対処するのか、モデルの成果をどう測るのかといった詰まりポイントを重点的に見ていきます。逆にモデル設計の詳細(畳み込み層の動きなど)は見ていきません。\n–\n実装の流れは以下の通りです。\n\n必要なライブラリのインストール\nデータセットのインストール\nトレーニングデータとテストデータの差異を確認\nサンプルデータ上でのトレーニング\nトレーニング結果の妥当性を確認\nデータセット全体でのトレーニング\nまとめ\n参照ページ\n\n\n以下のコードは実行環境にCUDA(GPU)が使用可能かを確認します。このノートはGPUが使えることを前提に動いています。kaggleの上でも無料でGPUが使えますが、クラウドのサービス(paperspace, google colabなど)の方がモデルの学習を早くできるのでお勧めです。\n\n!nvidia-smi\n\nThu Oct 31 02:47:40 2024       \n+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 525.116.04   Driver Version: 525.116.04   CUDA Version: 12.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  NVIDIA RTX A4000    Off  | 00000000:00:05.0 Off |                  Off |\n| 41%   37C    P8    16W / 140W |      1MiB / 16376MiB |      0%      Default |\n|                               |                      |                  N/A |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n\n\n\n\nfastkaggleはfastaiとkaggle apiを合わせたライプラリーです。ここで用いられている全インポートimport *は、fastai推奨のインポート方法です。私自身、初めは慣れずに居心地が悪い表記方法でしたが今は慣れました。jupyter notebookなどの実験(仮説と検証を繰り返す)環境では全インポートで問題ありません。\n\ntry: import fastkaggle\nexcept ModuleNotFoundError:\n    !pip install -Uq fastkaggle\n\nfrom fastkaggle import *\nfrom fastai.vision.all import *\n\nWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\n\n\n\n\n\n\n\nまずkaggleのコンペ参加には同意が必要になります。同意が済んだら、次はapiキーの取得です。/root/.config/kaggle/kaggle.jsonファイルに取得したapiキーを記述します。ファイルは以下のコードで作成します。\n\n!mkdir /root/.config/kaggle\n!touch /root/.config/kaggle/kaggle.json\n\n\n\n\nsetup_comp関数でデータセットをdigit-recognizerフォルダーにインストールします。ここでエラーが出た場合は、もう一度apiキーが設定されているか、競技の同意が済んでいるかを見てみます。よくあるエラー原因は競技名のスペルミスです。\n\ncomp = \"digit-recognizer\"\n\npath = setup_comp(comp)\npath.ls()\n\nWarning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\nDownloading digit-recognizer.zip to /notebooks\n\n\n100%|██████████| 15.3M/15.3M [00:00&lt;00:00, 74.5MB/s]\n\n\n\n\n\n(#3) [Path('digit-recognizer/sample_submission.csv'),Path('digit-recognizer/test.csv'),Path('digit-recognizer/train.csv')]\n\n\nトレーニングデータは42000、テストデータは28000サンプルあることがわかります。\n\ntrn_df = pd.read_csv(path/\"train.csv\")\ntst_df = pd.read_csv(path/\"test.csv\")\nsmp_df = pd.read_csv(path/\"sample_submission.csv\")\n\ntrn_df.shape, tst_df.shape, smp_df.shape\n\n((42000, 785), (28000, 784), (28000, 2))\n\n\n\n\n\n最初の行(label行)にはデータのラベルが、その他の行(pixel0 - 783行)にはピクセルデータが格納されていることがわかります。このデータは色(color channel)の無い白黒画像であることが推測できます。\n\ntrn_df.head(n=3)\n\n\n\n\n\n\n\n\nlabel\npixel0\npixel1\npixel2\npixel3\npixel4\npixel5\npixel6\npixel7\npixel8\n...\npixel774\npixel775\npixel776\npixel777\npixel778\npixel779\npixel780\npixel781\npixel782\npixel783\n\n\n\n\n0\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n1\n0\n0\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n3 rows × 785 columns\n\n\n\nラベルは0から9の10ラベルです。それぞれ約4000個と、おおよそ均等に配分されたデータセットであることが分かります。\n\ntrn_df[\"label\"].unique()\n\narray([1, 0, 4, 7, 3, 5, 8, 9, 2, 6])\n\n\n\ntrn_df.groupby(\"label\").size().plot.barh();\n\n\n\n\n\n\n\n\nラベルとピクセルデータ(画像)が対応していることを確認します。\n\nidx = 0\nlabel, img_val = trn_df.iloc[idx, 0], trn_df.iloc[idx, 1:].values\n\nshow_image(img_val.reshape((28, 28)), title=label);\n\n\n\n\n\n\n\n\n\n\n\ntrainとtestフォルダそれぞれに画像データを保存します。\n\n!mkdir {path}/train\n!mkdir {path}/test\npath.ls()\n\n(#5) [Path('digit-recognizer/train'),Path('digit-recognizer/sample_submission.csv'),Path('digit-recognizer/test.csv'),Path('digit-recognizer/train.csv'),Path('digit-recognizer/test')]\n\n\n\ndef save_img(img, f_path):\n    img = img.reshape((28, 28))\n    img = Image.fromarray(img.astype(\"uint8\"))\n    img.save(f_path)\n\n\nfor idx, row in trn_df.iterrows():\n    label, img = row[0], row[1:]\n\n    f_path = f\"{path}/train/{label}_{idx}.jpg\"\n    save_img(img.values, f_path)\n\n\n(path/\"train\").ls()\n\n(#42000) [Path('digit-recognizer/train/7_19624.jpg'),Path('digit-recognizer/train/2_40612.jpg'),Path('digit-recognizer/train/3_32852.jpg'),Path('digit-recognizer/train/1_13520.jpg'),Path('digit-recognizer/train/9_29764.jpg'),Path('digit-recognizer/train/6_16260.jpg'),Path('digit-recognizer/train/9_18518.jpg'),Path('digit-recognizer/train/0_13218.jpg'),Path('digit-recognizer/train/5_32574.jpg'),Path('digit-recognizer/train/7_20089.jpg')...]\n\n\n\nfor idx, img in tst_df.iterrows():\n    f_path = f\"{path}/test/{idx}.jpg\"\n    save_img(img.values, f_path)\n\n\n(path/\"test\").ls()\n\n(#28000) [Path('digit-recognizer/test/13420.jpg'),Path('digit-recognizer/test/22873.jpg'),Path('digit-recognizer/test/6022.jpg'),Path('digit-recognizer/test/13644.jpg'),Path('digit-recognizer/test/1332.jpg'),Path('digit-recognizer/test/3737.jpg'),Path('digit-recognizer/test/617.jpg'),Path('digit-recognizer/test/10239.jpg'),Path('digit-recognizer/test/14546.jpg'),Path('digit-recognizer/test/4200.jpg')...]\n\n\n\n\n\n\nまず分類モデルを作る前に、二つのデータセット(トレーニング・テスト)に違いがないこと確認します。例えばトレーニングセットには0から9までの数字画像が、テストセットには10から19までの数字といった全く違う種類のデータが入っているかもしれません。二つのデータセットに差異がないことは、この後の分類モデルを作る際の前提になります。 二つのデータセットの差異確認は深層学習モデルを用いて行います。ここでのラベルはtrain/testとなり、モデルは画像がどちらのデータセットに属しているのかを予測します。全データセット70000に対して、トレーニングは42000、テストは28000あります。もし、モデルが0.6 accuracy(42000/70000=0.6)から離れた値を出す場合、二つのデータセットは異なる種類のものであると推測できます。\n\ndef label_func(f_path): return f_path.parent.name\n\nlabel_func((path/\"train\").ls()[0]), label_func((path/\"test\").ls()[0])\n\n('train', 'test')\n\n\n\ndef get_db(size=28):\n    return DataBlock(\n        blocks=(ImageBlock, CategoryBlock),\n        get_items=get_image_files,\n        get_y=label_func,\n        splitter=RandomSplitter(),\n        item_tfms=Resize(size),\n        batch_tfms=[*aug_transforms(do_flip=False, min_scale=0.8), Normalize.from_stats(*imagenet_stats)]\n    )\n\nmnist = get_db()\n\n学習用のデータセットを用意する場合、全トレーニングデータを使う必要はありません。アイデアの有効性を試す実験段階では、開発サイクルを高速で回すためにも少ないサンプル数を使用します。 ここではランダムに、トレーニングデータセットから10%のデータを取得しています。どのようにサンプルデータを用意するのかは様々な手法がありますが、今回のような画像分類モデルでは、各ラベルのサンプル数が300ほどあれば実験用途として十分な気がします。以下のdls.show_batchによる画像の表示は、ざっとデータを理解するのに役立ちます。トレーニングの前段階で、トレーニングデータ(画像+対応するラベル)を確認するのは必須ステップです。\n\ndef get_dls(db, path, sample=1, bs=256):\n    # https://knowing.net/posts/2022/05/fastai-dataloaders/\n    dls = db.dataloaders(path, bs=bs)\n    selected_items = random.sample(dls.train_ds.items,  int(len(dls.train_ds.items)*sample))\n    dls.train = dls.test_dl(selected_items, with_labels=True)\n    return dls\n\ndls = get_dls(mnist, path, sample=0.1)\ndls.show_batch(max_n=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\nモデルの設計は入力/出力を中心に行います。fastaiはPytorchの上に作られたライブラリーなので、画像型は(N, C, H, W)、つまり(バッチ数, カラーチャンネル, 縦, 横)としてモデルに入力されます。モデルの出力はデータセットのラベル数dls.cによって決まります。ここではモデルは画像がトレーニング・テストセットのどちらに属しているのかを予測します。\n\ndef get_model():\n    return nn.Sequential(\n        nn.Conv2d(3, 1, 1),\n        # nn.Upsample(size=(28, 28)), # Needs Upsample layer if you'll use learn.tta() later\n        nn.Flatten(1),\n        nn.Dropout(0.1),\n        nn.Linear(28*28, 100),\n        nn.BatchNorm1d(100),\n        nn.Dropout(0.2),\n        nn.ReLU(),\n        nn.Linear(100, dls.c)\n    )\n\nget_model()\n\nSequential(\n  (0): Conv2d(3, 1, kernel_size=(1, 1), stride=(1, 1))\n  (1): Flatten(start_dim=1, end_dim=-1)\n  (2): Dropout(p=0.1, inplace=False)\n  (3): Linear(in_features=784, out_features=100, bias=True)\n  (4): BatchNorm1d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (5): Dropout(p=0.2, inplace=False)\n  (6): ReLU()\n  (7): Linear(in_features=100, out_features=2, bias=True)\n)\n\n\n学習に必要なデータセットの用意・モデルの設計が済んだら、いよいよトレーニングの開始です。learn.lr_find関数で学習率を求めたらトレーニングサイクルを回します。\n\nlearn = Learner(dls, get_model(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.015848932787775993)\n\n\n\n\n\n\n\n\n\n結果はおおよそ59% accuracyで、画像がどちらのデータセットに属しているのかを予測しています。二つのデータセットに差異はなさそうです。\n\nlearn.fit_one_cycle(4, lr_max=slice(3e-3, 3e-2))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.710549\n0.795406\n0.597000\n00:13\n\n\n1\n0.700336\n0.704644\n0.568500\n00:12\n\n\n2\n0.688758\n0.678533\n0.596929\n00:12\n\n\n3\n0.674994\n0.681112\n0.586357\n00:11\n\n\n\n\n\n\n\n\nでは、本題の画像分類モデルのトレーニングに移っていきます。サンプルデータの用意・画像の表示を行い、モデルの学習率を求めます。先のデータと同じ画像データを用いますが、今回は対応するラベルが異なります。ここでは、モデルは画像が0から9までのどの数字であるかを予測します。\n\ndef label_func(f_path): return f_path.name.split(\"_\")[0]\n\n(path/\"train\").ls()[0], label_func((path/\"train\").ls()[0])\n\n(Path('digit-recognizer/train/7_19624.jpg'), '7')\n\n\n\nmnist = get_db()\ndls = get_dls(mnist, path/\"train\", sample=0.1)\ndls.show_batch(max_n=30, nrows=3, ncols=10)\n\n\n\n\n\n\n\n\n\nlearn = Learner(dls, get_model(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0014454397605732083)\n\n\n\n\n\n\n\n\n\nおおよそ0.91 accuracyの精度のモデルを、10%のトレーニングデータで作りました。この時点でトレーニングデータを増やし、モデルの精度を高めようとする方法がありますが、これは間違いです。トレーニングデータ量を増やすことは過学習を防ぐのに有効ですが、学習不足には違う対処が求められます。 まずは他のモデルの精度と比較して、学習済みのモデルが過学習か学習不足かを見ていきます。ここではresnet18を比較モデルとします。\n\nlearn.fit_one_cycle(6, lr_max=slice(3e-3, 1e-2))\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.595407\n1.169721\n0.635357\n00:07\n\n\n1\n1.004385\n0.500168\n0.851667\n00:03\n\n\n2\n0.707560\n0.360822\n0.888214\n00:03\n\n\n3\n0.542243\n0.318499\n0.904405\n00:03\n\n\n4\n0.434244\n0.306300\n0.910000\n00:03\n\n\n5\n0.363178\n0.299947\n0.909881\n00:03\n\n\n\n\n\n\nlearn = Learner(dls, resnet18(), loss_func=CrossEntropyLossFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0010000000474974513)\n\n\n\n\n\n\n\n\n\n先のモデルと比較して、resnet18はより精度の高いモデルであることが分かります。モデル精度の比較結果から、先のモデルは学習不足であったことが分かります。一般的に、過学習になる前段階で注意するポイントは学習不足です。学習不足にはより複雑なモデルが有効です。トレーニングサイクルを増やす、学習率を少し上げるなども学習不足に効果的です。よくある間違いは、過学習になる前(学習不足段階)に過学習を防ぐ手法(データ量を増やす、データ拡張、regularizationなど)を用いることです。過学習への対処は、過学習が起きた後に行います。起きる前、予防的には行いません。\n\nlearn.fine_tune(6, 3e-3)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n2.481695\n1.180103\n0.615119\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n0.227101\n0.339515\n0.909167\n00:03\n\n\n1\n0.168335\n0.627921\n0.838571\n00:04\n\n\n2\n0.143743\n0.432962\n0.900833\n00:03\n\n\n3\n0.120995\n0.268689\n0.937381\n00:03\n\n\n4\n0.091097\n0.148554\n0.959881\n00:03\n\n\n5\n0.066370\n0.122915\n0.967143\n00:03\n\n\n\n\n\nトレーニング・検証データそれぞれの損失を見てきます。トレーニングデータの損失はスムーズな減少が見られますが、検証データの損失はスムーズではない形で推移しています。一つ考えられる原因は、重みの更新が急であることです。スムーズな損失の推移には、いくつかの正則化テクニックが効果的です。weight decayを後のトレーニングで試していきたいと思います。\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n\n学習済みのモデルが数字を正しく予測しているか、検証画像(学習していないデータ)を用いて見ていきます。おおよそ、モデルは正しく手書き画像を分類できていることが分かります。\n\nlearn.show_results(max_n=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n逆に、モデルがどの数字画像を正しく予測できていないかを見ていきます。2と7、4と9の数字ペアの分類が、モデルの最も不得意とする画像であることが分かります。\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.most_confused(min_val=15)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[('2', '7', 20), ('4', '9', 19)]\n\n\nここではモデルが実際に間違えた予測をしている画像を見ていきます。ざっくりと見た時、モデルが間違えた画像は人間が見ても間違えそうな、紛らわしい数字であることが見て取れます。この紛らわしいラベルへの対処方としては、label smoothingの導入があります。\n\ninterp.plot_top_losses(k=20, nrows=2, ncols=10)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nでは先のトレーニング結果を踏まえて、再度トレーニングをしていきます。モデル・データセットは同様に、label smoothing・weight decayを新たに使用します。label smoothing関数はラベルのノイズを抑え損失を滑らかにしますが、より多くのトレーニングサイクルを必要とします。先のトレーニングとの相違点は、accuracyの精度は上がっている一方、lossが以前より高い値であることです。ここでの注意点は、accuracyの精度を見ること・lossは参考程度に見ることです。lossはモデルがトレーニングしやすいように設計された関数の出力です。見るべき大事なポイント(モデルの精度)はaccuracyです。\n\nlearn = Learner(dls, resnet18(), loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy).to_fp16()\nlearn.fine_tune(9, 2e-3, wd=0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n3.378253\n2.647140\n0.526190\n00:03\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.336900\n1.490081\n0.937024\n00:03\n\n\n1\n1.207181\n1.428404\n0.939048\n00:03\n\n\n2\n1.151014\n1.397945\n0.923333\n00:03\n\n\n3\n1.131866\n1.395567\n0.904762\n00:03\n\n\n4\n1.111709\n1.244161\n0.941310\n00:03\n\n\n5\n1.094381\n1.210672\n0.945476\n00:03\n\n\n6\n1.079392\n1.151900\n0.958809\n00:03\n\n\n7\n1.064843\n1.112762\n0.970357\n00:03\n\n\n8\n1.053271\n1.109932\n0.971310\n00:03\n\n\n\n\n\nモデルの精度・loss推移の滑らかさともに向上しています。\n\nlearn.recorder.plot_loss()\n\n\n\n\n\n\n\n\n\n\n今度は全トレーニングデータを用いて学習サイクルを回します。ここでのポイントは、ぼーっと学習の進行を見ない・ノートブックから離れることです。\n\ndls = get_dls(mnist, path/\"train\", sample=1) # Use all training image samples\nlearn = Learner(dls, resnet18(), loss_func=LabelSmoothingCrossEntropyFlat(), metrics=accuracy).to_fp16()\nlearn.lr_find()\n\n\n\n\n\n\n\n\nSuggestedLRs(valley=0.0006918309954926372)\n\n\n\n\n\n\n\n\n\n\nlearn.fine_tune(15, 1e-3, wd=0.03)\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.352898\n1.110789\n0.974286\n00:32\n\n\n\n\n\n\n\n\n\n\n\n\nepoch\ntrain_loss\nvalid_loss\naccuracy\ntime\n\n\n\n\n0\n1.050016\n1.066425\n0.983810\n00:33\n\n\n1\n1.039457\n1.074759\n0.982262\n00:33\n\n\n2\n1.050062\n1.090605\n0.977381\n00:33\n\n\n3\n1.048530\n1.073222\n0.982857\n00:35\n\n\n4\n1.041486\n1.075171\n0.981310\n00:34\n\n\n5\n1.038663\n1.071839\n0.983929\n00:35\n\n\n6\n1.031112\n1.061485\n0.986905\n00:36\n\n\n7\n1.026435\n1.066055\n0.985714\n00:34\n\n\n8\n1.025973\n1.065412\n0.985833\n00:34\n\n\n9\n1.020517\n1.052722\n0.990000\n00:35\n\n\n10\n1.018529\n1.049975\n0.989643\n00:35\n\n\n11\n1.016169\n1.046174\n0.991429\n00:34\n\n\n12\n1.015739\n1.046068\n0.991071\n00:34\n\n\n13\n1.015518\n1.045375\n0.991548\n00:40\n\n\n14\n1.015467\n1.045369\n0.991548\n00:42\n\n\n\n\n\n\n\n\nでは、学習済みモデルの精度をテストデータで評価していきます。まずはテストファイルを名前順にソートし、テスト画像(ラベルは含まれていない)を表示します。learn.dls.test_dlは、トレーニング画像と同じ下処理を、テスト画像に行う便利な関数です。\n\n# https://stackoverflow.com/questions/33159106/sort-filenames-in-directory-in-ascending-order\nordered_tst_files = get_image_files(path/\"test\").sorted(key=lambda f:  int(re.findall(\"(\\d+).jpg$\", f.name)[0]))\nordered_tst_files[:10]\n\n(#10) [Path('digit-recognizer/test/0.jpg'),Path('digit-recognizer/test/1.jpg'),Path('digit-recognizer/test/2.jpg'),Path('digit-recognizer/test/3.jpg'),Path('digit-recognizer/test/4.jpg'),Path('digit-recognizer/test/5.jpg'),Path('digit-recognizer/test/6.jpg'),Path('digit-recognizer/test/7.jpg'),Path('digit-recognizer/test/8.jpg'),Path('digit-recognizer/test/9.jpg')]\n\n\n\ntest_dl = learn.dls.test_dl(ordered_tst_files)\ntest_dl.show_batch(max_n=10, nrows=1, ncols=10)\n\n\n\n\n\n\n\n\n学習済みモデルにテスト画像を予測(分類)させます。\n\npreds = learn.get_preds(dl=test_dl, with_decoded=True)\npreds\n\n\n\n\n\n\n\n\n(tensor([[7.9680e-05, 5.4073e-05, 8.9991e-01,  ..., 9.6678e-05, 9.9019e-05,\n          1.0097e-04],\n         [8.9885e-01, 7.8046e-05, 1.3502e-04,  ..., 1.0204e-04, 1.0433e-04,\n          9.8660e-05],\n         [7.4821e-05, 1.0368e-04, 1.0127e-04,  ..., 1.0342e-04, 1.0242e-04,\n          1.0302e-04],\n         ...,\n         [6.9998e-05, 5.2941e-05, 2.7252e-05,  ..., 1.0606e-04, 1.0679e-04,\n          9.8137e-05],\n         [9.9981e-05, 8.6992e-05, 9.6574e-05,  ..., 1.0136e-04, 1.0376e-04,\n          9.7474e-05],\n         [5.9522e-05, 4.7224e-05, 9.0107e-01,  ..., 8.9877e-05, 9.8856e-05,\n          9.8278e-05]]),\n None,\n tensor([2, 0, 9,  ..., 3, 9, 2]))\n\n\n予測した数字が妥当かどうか、画像と一緒に見比べてみます。モデルはテスト画像の数字を分類できていそうです。\n\nprint(preds[2][:10])\ntest_dl.show_batch(max_n=10, nrows=1, ncols=10)\n\ntensor([2, 0, 9, 0, 3, 7, 0, 3, 0, 3])\n\n\n\n\n\n\n\n\n\n予測した値をcsvファイルに保存し、kaggleに提出します。\n\nsmp_df[\"Label\"] = preds[2].apply_(lambda x: int(learn.dls.vocab[x]))\nsmp_df.to_csv(path/\"submission.csv\", index=False)\n!head {path}/submission.csv\n\nImageId,Label\n1,2\n2,0\n3,9\n4,0\n5,3\n6,7\n7,0\n8,3\n9,0\n\n\n\nfrom kaggle import api\napi.competition_submit_cli(path/\"submission.csv\", \"first model\", comp)\n\n100%|██████████| 208k/208k [00:00&lt;00:00, 657kB/s]\n\n\nSuccessfully submitted to Digit Recognizer\n\n\n最後にインストールしたzipファイルと画像ファイルを削除します。\n\n!rm -rf {path}\n!rm -rf digit-recognizer.zip\n\n\n\n\n\n\n\n\nkaggle の結果\n\n\n目標値99% accuracyを達成することができました。今回のポイント三点をまとめていきます。\n\n少ないサンプル数を使用して、高速な開発サイクルを回す。 → 全サンプルを使用するのは、一番最後に。\n過学習の対処は過学習が起きた後に行う。 → 予防的には行わない。先に注意するのは学習不足とその対処。\nモデル精度の評価はメトリックを見る。 → 損失の推移は参考程度に。\n\nもしも読者がこのノートを役に立ったと思ったら、リアクションボタンを押してもらえると幸いです。質問や間違いがあれば、以下コメント欄に書き込んでください。\n\n\n\n\nkaggle公式競技ページ\nmnistデータセットのwiki\nfastaiのクリエイター(Jeremy Howardさん)のノートブック\n他の競技参加者(Jacopo Repossiさん)のノートブック"
  }
]